{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp-url-html.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeAOm-QWmKwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk # Natural Language Toolkit\n",
        "import string # Works with strings\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize # Word and sentence tokenizer\n",
        "from nltk.corpus import stopwords # Works with stopwords\n",
        "\n",
        "nltk.download('punkt') # Helps to divides a text into a list of sentences\n",
        "nltk.download('stopwords') # Helps to get stopwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kUgY2otmQ9a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example of word tokenizing\n",
        "\n",
        "data = \"Example of a sentence, which we will separate the words\"\n",
        "words = word_tokenize(data)\n",
        "print(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKqRsXWmmREs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Getting the working path\n",
        "\n",
        "import os\n",
        "print(os.getcwd())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJeFk0r4mRIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# File reading and word tokenizing\n",
        "\n",
        "lyrics = open('futureTense.txt', \"r\")\n",
        "x = lyrics.read()\n",
        "lyrics.close()\n",
        "print(x)\n",
        "\n",
        "words = word_tokenize(x)\n",
        "words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GcZQgStmRRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Filtering all stopwords and getting the punctuactions\n",
        "\n",
        "wordsFiltered = []\n",
        "stopWords = set(stopwords.words('english'))\n",
        "punctuations = list(string.punctuation)\n",
        "for w in words:\n",
        "    if w not in stopWords and w not in punctuations:\n",
        "        wordsFiltered.append(w)    \n",
        "print(wordsFiltered)\n",
        "\n",
        "# Retired terms\n",
        "\n",
        "terms = set(words)\n",
        "terms = list(terms)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDyZCKyumd8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Library to work with url opening\n",
        "\n",
        "import urllib.request\n",
        "response = urllib.request.urlopen('https://www.letras.mus.br/oasis/28995/')\n",
        "html = response.read()\n",
        "print (html)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bem4ed_Amd_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Library to filter HTML\n",
        "\n",
        "from bs4 import BeautifulSoup \n",
        "import urllib.request \n",
        "response = urllib.request.urlopen('https://www.letras.mus.br/oasis/28995/')\n",
        "\n",
        "html = response.read() \n",
        "soup = BeautifulSoup(html,\"html5lib\") \n",
        "text = soup.get_text(strip=True) \n",
        "tokens = [t for t in text.split()] \n",
        "print (tokens)\n",
        "\n",
        "soup = BeautifulSoup(html, \"lxml\")\n",
        "text = soup.find(id=\"js-lyric-cnt\").text\n",
        "\n",
        "text\n",
        "\n",
        "words = word_tokenize(text)\n",
        "words\n",
        "\n",
        "wordsFiltered = []\n",
        "\n",
        "stopWords = set(stopwords.words('english'))\n",
        "punctuations = list(string.punctuation)\n",
        "for w in words:\n",
        "    if w not in stopWords and w not in punctuations:\n",
        "        wordsFiltered.append(w)    \n",
        "print(wordsFiltered)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}